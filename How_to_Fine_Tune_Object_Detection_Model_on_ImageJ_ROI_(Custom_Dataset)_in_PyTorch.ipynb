{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First,install package roifile to read the data of ImageJ roi files,\n",
        "github address：https://github.com/cgohlke/roifile/tree/master"
      ],
      "metadata": {
        "id": "srDROvPolcWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U roifile[all]"
      ],
      "metadata": {
        "id": "JQUYrr721br5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YlYGucJ1OG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from roifile import ImagejRoi, ROI_TYPE, ROI_OPTIONS\n",
        "from bs4 import BeautifulSoup\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import files\n",
        "import io\n",
        "import cv2\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How roifile package read the roi data from ImageJ roi file\n",
        "roi_instance = ImagejRoi.fromfile('/content/drive/MyDrive/object_detection/archive/annotations_imagej_roi/maksssksksss0.zip')\n",
        "print(roi_instance)"
      ],
      "metadata": {
        "id": "PN-kl4QMYmTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the function to read the images and annotation from ImageJ roi files and convert to the input format in pytorch\n",
        "def generate_box(obj):\n",
        "  xmin = obj.left\n",
        "  ymin = obj.top\n",
        "  xmax = obj.right\n",
        "  ymax = obj.bottom\n",
        "  return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "def generate_label(obj):\n",
        "  if not isinstance(obj.name, str):\n",
        "    return 0\n",
        "  if 'with_mask' in obj.name:\n",
        "      return 1\n",
        "  elif 'mask_weared_incorrect' in obj.name:\n",
        "      return 2\n",
        "  elif 'without_mask' in obj.name:\n",
        "      return 3\n",
        "  else:\n",
        "      return 0\n",
        "\n",
        "def generate_target(image_id, roi_file_path):\n",
        "  # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "  boxes = []\n",
        "  labels = []\n",
        "  roi_objects = ImagejRoi.fromfile(roi_file_path)\n",
        "  for i in roi_objects:\n",
        "    boxes.append(generate_box(i))\n",
        "    labels.append(generate_label(i))\n",
        "\n",
        "  boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "  labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "  img_id = torch.tensor([image_id])\n",
        "  # Annotation is in dictionary format\n",
        "  target = {}\n",
        "  target[\"boxes\"] = boxes\n",
        "  target[\"labels\"] = labels\n",
        "  target[\"image_id\"] = img_id\n",
        "\n",
        "  return target"
      ],
      "metadata": {
        "id": "NJ0kMjL13DTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskDataset(object):\n",
        "    def __init__(self, img_path, annot_roi_path, transforms):\n",
        "      self.img_path = img_path\n",
        "      self.annot_roi_path = annot_roi_path\n",
        "      self.transforms = transforms\n",
        "      self.imgs = list(sorted(os.listdir(img_path)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      files = os.listdir(self.img_path)\n",
        "      file_image = files[idx]\n",
        "      file_image_name = file_image.split('.')[0]\n",
        "      file_annotation = file_image_name + '.zip'\n",
        "      img_path = os.path.join(self.img_path, file_image)\n",
        "      annotation_path = os.path.join(self.annot_roi_path, file_annotation)\n",
        "      img = Image.open(img_path).convert(\"RGB\")\n",
        "      target = generate_target(idx, annotation_path)\n",
        "\n",
        "      if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "\n",
        "      return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.imgs)"
      ],
      "metadata": {
        "id": "EUPwLVYH3Hrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([transforms.ToTensor(), ])"
      ],
      "metadata": {
        "id": "8Bwj7UCq3Lss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))\n",
        "\n",
        "#The dataset used is a public dataset to detect the mask, the original anntation format is xml.\n",
        "#Converted the annotation files to ImageJ ROI format For showing how to fine tune object detection model on ImageJ ROI (Custom Dataset)\n",
        "\n",
        "images_path = \"/content/drive/MyDrive/object_detection/archive/images/\"\n",
        "annotations_path = \"/content/drive/MyDrive/object_detection/archive/annotations_imagej_roi/\"\n",
        "\n",
        "dataset = MaskDataset(images_path,annotations_path,data_transform)\n",
        "\n",
        "# Split the dataset，adjust the validation and test set size as needed\n",
        "#val_size = int(0.2 * len(dataset))\n",
        "test_size = int(0.01 * len(dataset))\n",
        "train_size = len(dataset) - test_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print('length of training dataset: ', len(train_dataset), '\\n', 'length of testing dataset: ', len(test_dataset))\n",
        "\n",
        "#Create Data Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,  num_workers=2, collate_fn=collate_fn)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True,  num_workers=2, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,  num_workers=2, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "zvAruTQi3Mku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_objective_detection(num_classes):\n",
        "  # load an instance object detection model pre-trained\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "IDNTSwkH3RZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_objective_detection(4)"
      ],
      "metadata": {
        "id": "AqY-04Ii3SUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "H5rcuj5V3Smk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the visualisation function to check whether the input is right\n",
        "def plot_image(imgs, annotations):\n",
        "  fig,ax = plt.subplots(1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  img = imgs.cpu().numpy()\n",
        "  boxes = annotations[\"boxes\"].detach().cpu().numpy()\n",
        "  labels = annotations[\"labels\"].detach().cpu().numpy()\n",
        "  # Display the image\n",
        "  ax.imshow(np.transpose(img,(1, 2, 0)))\n",
        "\n",
        "  for box,label in zip(boxes, labels):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    # Create a Rectangle patch\n",
        "    if label == 1:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-15),class_name,verticalalignment='top',color='r',fontsize=5,weight='bold')\n",
        "    elif label == 2:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-15),class_name,verticalalignment='top',color='b',fontsize=5,weight='bold')\n",
        "    else:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-15),class_name,verticalalignment='top',color='g',fontsize=5,weight='bold')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Mxeit4LR3TE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select one image and its annotation to check whether the input is right by visual way\n",
        "for imgs, annotations in train_loader:\n",
        "  imgs = list(img.to(device) for img in imgs)\n",
        "  annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "  print(imgs[0].shape,'\\n',annotations)\n",
        "  break\n",
        "\n",
        "class_names = ['background', 'with_mask', 'mask_weared_incorrect', 'without_mask']\n",
        "print(\"Target\")\n",
        "plot_image(imgs[0], annotations[0])"
      ],
      "metadata": {
        "id": "exE4rAhY3TVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start training\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 150\n",
        "\n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "len_dataloader = len(train_loader)\n",
        "\n",
        "history = []\n",
        "\n",
        "# Initialize variables to track best validation loss\n",
        "best_loss = float('inf')\n",
        "# Define a path to save the best model\n",
        "best_model_path = '/content/drive/MyDrive/object_detection/archive/model_1.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = time.time()\n",
        "    model.train()\n",
        "    i = 0\n",
        "    training_loss = 0\n",
        "    #valid_loss = 0\n",
        "    for imgs, annotations in train_loader:\n",
        "      i += 1\n",
        "      imgs = list(img.to(device) for img in imgs)\n",
        "      annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "      loss_dict = model([imgs[0]], [annotations[0]])\n",
        "      losses = sum(loss for loss in loss_dict.values())\n",
        "      training_loss += losses\n",
        "      optimizer.zero_grad()\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "      #print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
        "    #with torch.no_grad():\n",
        "      # Set to evaluation mode\n",
        "      #model.eval()\n",
        "    # Validation loop\n",
        "      #j = 0\n",
        "      #for imgs, annotations in val_loader:\n",
        "        #j += 1\n",
        "        #imgs = list(img.to(device) for img in imgs)\n",
        "        #annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "        #valid_loss_dict = model([imgs[0]], [annotations[0]])\n",
        "        #losses = sum(loss for loss in valid_loss_dict.values())\n",
        "        #valid_loss += losses\n",
        "    # Update the learning rate based on valid_loss\n",
        "    #scheduler.step()\n",
        "    history.append([training_loss])\n",
        "      # Save the best performance model in validation with least valid loss over all the epochs\n",
        "    if training_loss < best_loss:\n",
        "      best_loss = training_loss\n",
        "      torch.save(model.state_dict(), best_model_path)\n",
        "    epoch_end = time.time()\n",
        "    #Print the training and validation history\n",
        "    print(\"Epoch: {}/{},  Training: Loss: {:.4f},  Time: {:.4f}s\".format(epoch+1, num_epochs, training_loss, epoch_end-epoch_start))"
      ],
      "metadata": {
        "id": "Z7OL2_Lo5iOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_loss)\n",
        "# Visualize the training history\n",
        "train_loss = [row[0].detach().cpu() for row in history]\n",
        "#valid_loss = [row[1] for row in history]\n",
        "plt.plot(train_loss, label='Train loss')\n",
        "#plt.plot(valid_loss,label='Valid loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss curve for training')\n",
        "plt.grid(True)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJFB2LP95ip-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the best model saved previous\n",
        "best_model_path = '/content/drive/MyDrive/object_detection/archive/model_1.pth'\n",
        "model.load_state_dict(torch.load(best_model_path))"
      ],
      "metadata": {
        "id": "4-IXluzaGJHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the test dataset\n",
        "all_imgs = []\n",
        "all_annotations = []\n",
        "\n",
        "for imgs, annotations in test_loader:\n",
        "  imgs = list(img.to(device) for img in imgs)\n",
        "  annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "  all_imgs.extend(imgs)\n",
        "  all_annotations.extend(annotations)\n",
        "\n",
        "print(all_annotations)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "preds = model(all_imgs)\n",
        "preds"
      ],
      "metadata": {
        "id": "CNknTmDK5jFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the function to visualise the truth and prediction of test dataset\n",
        "class_names = ['background', 'with_mask', 'mask_weared_incorrect', 'without_mask']\n",
        "def plot_pred_truth_image(imgs, preds, annotations):\n",
        "  fig,ax = plt.subplots(1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  img = imgs.cpu().numpy()\n",
        "  boxes_pred = preds[\"boxes\"].detach().cpu().numpy()\n",
        "  labels_pred = preds[\"labels\"].detach().cpu().numpy()\n",
        "  scores_pred = preds[\"scores\"].detach().cpu().numpy()\n",
        "  boxes = annotations[\"boxes\"].detach().cpu().numpy()\n",
        "  labels = annotations[\"labels\"].detach().cpu().numpy()\n",
        "  # Display the image\n",
        "  ax.imshow(np.transpose(img,(1, 2, 0)))\n",
        "\n",
        "  for box,label,score in zip(boxes_pred, labels_pred, scores_pred):\n",
        "    if score > 0 :\n",
        "      xmin, ymin, xmax, ymax = box\n",
        "    # Create a Rectangle patch\n",
        "      if label == 1:\n",
        "        ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='y',facecolor='none'))\n",
        "        class_name = class_names[label]\n",
        "        ax.text(xmin,(ymin-20),'{}{:.2f}'.format(class_name,score),verticalalignment='top',color='y',fontsize=6,weight='bold')\n",
        "      elif label == 2:\n",
        "        ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='y',facecolor='none'))\n",
        "        class_name = class_names[label]\n",
        "        ax.text(xmin,(ymin-20),'{}{:.2f}'.format(class_name,score),verticalalignment='top',color='y',fontsize=6,weight='bold')\n",
        "      else:\n",
        "        ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none'))\n",
        "        class_name = class_names[label]\n",
        "        ax.text(xmin,(ymin-20),'{}{:.2f}'.format(class_name,score),verticalalignment='top',color='b',fontsize=6,weight='bold')\n",
        "\n",
        "  for box,label in zip(boxes, labels):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    # Create a Rectangle patch\n",
        "    if label == 1:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-10),'',verticalalignment='top',color='r',fontsize=5,weight='bold')\n",
        "    elif label == 2:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-10),'',verticalalignment='top',color='r',fontsize=5,weight='bold')\n",
        "    else:\n",
        "      ax.add_patch(patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none'))\n",
        "      class_name = class_names[label]\n",
        "      ax.text(xmin,(ymin-10),'',verticalalignment='top',color='g',fontsize=5,weight='bold')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gqxa_gMy-4hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(preds)):\n",
        "  print(f'\\n Prediction {i+1}')\n",
        "  plot_pred_truth_image(all_imgs[i], preds[i], all_annotations[i])"
      ],
      "metadata": {
        "id": "S9dnAgYUAbbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the prediction evaluation package to evaluate of test dataset, github address: https://github.com/rafaelpadilla/Object-Detection-Metrics\n",
        "import sys\n",
        "# Add the directory containing your module to the Python path\n",
        "sys.path.append('/content/drive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "from _init_paths import *\n",
        "from utils import *\n",
        "from Evaluator import *\n",
        "from BoundingBox import *\n",
        "from BoundingBoxes import *\n"
      ],
      "metadata": {
        "id": "CGG3QprynRje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the function to get the bounding boxes from truth and prediction annotation for calculating the metrics in next step.\n",
        "def getBoundingBoxes(annotations, preds):\n",
        "  \"\"\"Read txt files containing bounding boxes (ground truth and detections).\"\"\"\n",
        "  allBoundingBoxes = BoundingBoxes()\n",
        "\n",
        "  for i in range(len(annotations)):\n",
        "    nameOfImage = 'img'+ str(i)\n",
        "    annotation = annotations[i]\n",
        "    boxes = annotation[\"boxes\"].detach().cpu().numpy()\n",
        "    labels = annotation[\"labels\"].detach().cpu().numpy()\n",
        "    for box,label in zip(boxes, labels):\n",
        "      xmin, ymin, xmax, ymax = box\n",
        "      idClass = class_names[label]  # class\n",
        "      x = xmin\n",
        "      y = ymin\n",
        "      w = xmax-xmin\n",
        "      h = ymax-ymin\n",
        "      bb = BoundingBox(nameOfImage,idClass,x,y,w,h,CoordinatesType.Absolute, (w, h),BBType.GroundTruth,format=BBFormat.XYWH)\n",
        "      allBoundingBoxes.addBoundingBox(bb)\n",
        "\n",
        "  for i in range(len(preds)):\n",
        "    nameOfImage = 'img'+ str(i)\n",
        "    pred = preds[i]\n",
        "    boxes_pred = pred[\"boxes\"].detach().cpu().numpy()\n",
        "    labels_pred = pred[\"labels\"].detach().cpu().numpy()\n",
        "    scores_pred = pred[\"scores\"].detach().cpu().numpy()\n",
        "    for box,label,score in zip(boxes_pred, labels_pred, scores_pred):\n",
        "      #every box in each image\n",
        "      if score > 0 :\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        idClass = class_names[label]  # class\n",
        "        x = xmin\n",
        "        y = ymin\n",
        "        w = xmax-xmin\n",
        "        h = ymax-ymin\n",
        "        bb = BoundingBox(nameOfImage,idClass,x,y,w,h,CoordinatesType.Absolute, (w, h),BBType.Detected,score,format=BBFormat.XYWH)\n",
        "        allBoundingBoxes.addBoundingBox(bb)\n",
        "  return allBoundingBoxes\n"
      ],
      "metadata": {
        "id": "x3nBZBGZ5kTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read bounding boxes (ground truth and detections)\n",
        "boundingboxes = getBoundingBoxes(all_annotations, preds)\n",
        "boundingboxes\n",
        "\n",
        "# Uncomment the line below to generate images based on the bounding boxes\n",
        "# createImages(dictGroundTruth, dictDetected)\n",
        "# Create an evaluator object in order to obtain the metrics\n",
        "evaluator = Evaluator()\n",
        "##############################################################\n",
        "# VOC PASCAL Metrics\n",
        "##############################################################\n",
        "# Plot Precision x Recall curve\n",
        "evaluator.PlotPrecisionRecallCurve(\n",
        "    boundingboxes,  # Object containing all bounding boxes (ground truths and detections)\n",
        "    IOUThreshold=0.7,  # IOU threshold\n",
        "    method=MethodAveragePrecision.EveryPointInterpolation,  # As the official matlab code\n",
        "    showAP=True,  # Show Average recision in the title of the plot\n",
        "    showInterpolatedPrecision=True)  # Plot the interpolated precision curve\n",
        "# Get metrics with PASCAL VOC metrics\n",
        "metricsPerClass = evaluator.GetPascalVOCMetrics(\n",
        "    boundingboxes,  # Object containing all bounding boxes (ground truths and detections)\n",
        "    IOUThreshold=0.7,  # IOU threshold\n",
        "    method=MethodAveragePrecision.EveryPointInterpolation)  # As the official matlab code\n",
        "print(\"Average precision values per class:\\n\")\n",
        "# Loop through classes to obtain their metrics\n",
        "for mc in metricsPerClass:\n",
        "    # Get metric values per each class\n",
        "    c = mc['class']\n",
        "    precision = mc['precision']\n",
        "    recall = mc['recall']\n",
        "    average_precision = mc['AP']\n",
        "    ipre = mc['interpolated precision']\n",
        "    irec = mc['interpolated recall']\n",
        "    # Print AP per class\n",
        "    print('%s: %f' % (c, average_precision))"
      ],
      "metadata": {
        "id": "ucsCTD_HDLxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_aDnollDPIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdqACKkhoQPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tc2YsxJVJua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSnOa57vVKIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}